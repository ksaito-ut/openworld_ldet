<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140332927-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-140332927-1');
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->

<title>LDET Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="images/ldet_teaser.gif"/>

<meta property="og:title" content="Learning to Detect Every Thing in an Open World"/>

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">

#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="b5m.js" id="b5mmain" type="text/javascript"></script>
</head>

<body>
<div id="primarycontent">
<center><h1>Learning to Detect Every Thing in an Open World </h1></center>
<center><h2>
	<a href="https://cs-people.bu.edu/keisaito/index.html">Kuniaki Saito</a><sup>1 </sup>&nbsp;&nbsp;&nbsp;
  <a href="http://cs-people.bu.edu/pinghu/homepage.html">Ping Hu</a><sup>1 </sup>&nbsp;&nbsp;&nbsp;
  <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>2 </sup>&nbsp;&nbsp;&nbsp;
	<a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a><sup>1, 3</sup>&nbsp;&nbsp;&nbsp;
	</h2>

	<center><h2>
		<a> 1. Boston University </a> &nbsp;&nbsp;&nbsp;
    <a> 2. University of California, Berkeley  </a> &nbsp;&nbsp;&nbsp;
		<a> 3. MIT-IBM Watson AI Lab </a> &nbsp;&nbsp;&nbsp;
	</h2></center>

<center><h2><strong><a href="paper.pdf">Paper</a>  </h2> </a></strong> </center>
<center>

<img src="images/ldet_teaser.gif" width="85%"> </a></center>

<br>
<h1 align="center">Abstract</h1>

<div style="font-size:14px"><p align="justify"> Many open-world applications require the detection of novel objects, yet state-of-the-art object detection and instance segmentation networks do not excel at this task. The key issue lies in their assumption that regions without any annotations should be suppressed as negatives, which teaches the model to treat the unannotated objects as background. To address this issue, we propose a simple yet surprisingly powerful data augmentation and training scheme we call <b>Learning to Detect Every Thing (LDET)</b>. To avoid suppressing hidden objects (background objects that are visible but unlabeled), we paste annotated objects on a background image sampled from a small region of the original image. Since training solely on such synthetically-augmented images suffers from domain shift, we decouple the training into two parts: 1) training the region classification and regression head on augmented images, and 2)~training the mask heads on original images. In this way, a model does not learn to classify hidden objects as background while generalizing well to real images.
 LDET leads to significant improvements on many datasets in the open-world instance segmentation task, outperforming baselines on cross-category generalization on COCO, as well as cross-dataset evaluation on UVO and Cityscapes. </p></div>


<a href=""><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper_thumbnail.png" width=170></a>



<h2>Paper</h2>
<p><a href="">arxiv</a>,  2021. </p>



<h2>Citation</h2>
<p>Kuniaki Saito, Ping Hu, Trevor Darrell, Kate Saenko.<br>"Learning to Detect Every Thing in an Open World
".
<a href="">Bibtex</a>

</p>


<!-- <h2>Code </h2> <p><a href=''> Coming Soon</a></p> -->

<br>


<br>

<br>
<h1 align='center'> Open World Instance Segmentation</h1>
<center><img class="img-responsive img-hover" src="images/detected_results_uvo.webp" width="1000"></center>
<p align="justify"> State-of-the-art object detectors are designed to detect objects, which are given annotations in dataset, while they do not excel at detecting novel objects. However, many applications require to localize objects though it may not be necessary to identify their categories. In the open world instance segmentation, the model needs to localize novel objects in the scene. </p>

<h1 align='center'> Challenge in Open World Instance Segmentation</h1>
	<center> <img src="images/motivate_hidden.webp" width="900" /></center> <br>

	<p align="justify">
		What limits the ability of detectors to locate novel objects? First, most datasets do not annotate all objects in a scene. The above is the images from COCO. The colored boxes are annotated while other regions are not. Note that there are many unlabeled objects (marked with white dashed boxes) in the scene. Then, the detectors are trained to suppress the unlabeled objects as "background". This discourages the detectors to detect novel objects.
	</p>
<h1 align='center'> Data Augmentation: Background Erasing </h1>
	<center> <img src="images/pipeline_synthesis.webp" width="700" /></center> <br>

	<p align="justify">
		First, to avoid suppressing unlabeled objects as background, we propose to erase them by using the mask of foreground objects. We paste foreground objects on a background image and train a detector on the synthesized image. Since we need a background image, which does not contain foreground objects, we propose to create it by cropping a patch from a small region of the input image. But, the synthesized images look totally different from real images, which makes the trained detector work poor on real images.
	</p>


<br>
<h1 align='center'> Decoupled Training</h1>
<center> <img src="images/training_figure.webp" width="500" /></center> <br>
<p align="justify">
  To handle the domain-shift between synthesized and real images, we propose to decouple the training into two parts: 1) training detector losses on synthesized images, 2) training mask loss on real images. In this way, the detector will not suppress unlabeled objects as background while it will generalize well to real images thanks to the mask loss.
</p>

<h1 align='center'> Experiments</h1>
<center> <img src="images/numerical_results.webp" width="1000" /></center> <br>
<p align="justify">
  This is the result of models trained on VOC categories of COCO (VOC-COCO) or whole COCO, and tested on <a href="https://arxiv.org/pdf/2104.04691.pdf">UVO</a>. LDET improves on Mask RCNN with a large margin in all metrics. Note that LDET trained on VOC-COCO is comparable to or better than Mask RCNN trained on COCO in AR.
</p>

<h1 align='center'>Visualization</h1>
<center> <img src="images/detected_results_coco.webp" width="1000" /></center> <br>
<p align="justify">
Models trained on VOC-COCO are tested on COCO validation images. LDET detects novel objects well e.g., <i>giraffe, trash box, pen, kite, and floats</i>.
</p>
<h1 align='center'>Objectness Visualization</h1>
<center> <img src="images/objectness.webp" width="800" /></center> <br>
<p align="justify">
Visualization of objectness score from region proposal network. Note that LDET predicts the score well while baseline suppresses the objectness of many objects.
</p>


</body></html
>
